agent:
  type: QLearning
  learning_rate: 0.1
  epsilon:
    start: 1.0
    decay: 0.00002
    final: 0.05
  discount_factor: 0.99
  obs_bins: [20, 20]
  action_bins: [1, 20]

env:
  max_steps: 3000
  dt: 0.01
  state_indices_to_keep: [4] # Keep only the relevant states for observation
  reference_config:
    1:                      # the state index
      type: cos_step        # choose: 'sin', 'constant', or 'cos_step'
      amplitude:
        min: -0.349        # minimum amplitude (degrees for cos_step, radians for sin)
        max: 0.349          # maximum amplitude (degrees for cos_step, radians for sin)
        n_levels: 15      # number of discrete levels (for cos_step)
      step_duration: 
        min: 5.0            # minimum duration of each reference segment (seconds)
        max: 5.0        # maximum duration of each reference segment (seconds)
        n_levels: 1
      transition_duration:
        min: 3.0           # minimum duration of gap between segments (seconds)
        max: 5.0            # maximum duration of gap between segments (seconds)
        n_levels: 3
      
      # Parameters for sin
      A: 0.35               # amplitude in radians
      T: 5.0                # period
      phi: 0.0              # phase
      
      # Parameters for constant
      value: 1.0            # constant value
      
      # Parameters for cos_step
      amp_range: [-20.0, 20.0]  # amplitude limits in degrees
      n_levels: 15               # discrete amplitude levels
      T_step: 5.0                # step duration


  obs_low: [-1.5, -2.5]
  obs_high: [1.5, 2.5]

  # Action space bounds
  action_low: [0, -22.5]
  action_high: [0, 27]


training: 
  episodes: 100
  seed: 42
  
  milestones_fractions:
    - 0.1
    - 0.5
    - 1.0
  
  checkpoint_interval: 10000
  resume_from: null  
  #resume_from: experiments\q_learning\pitch_rate_and_theta_error_obs\checkpoint_final_ep60100\checkpoint_ep60099_brain.npz # null OR .npz file path
  

eval:
  rolling_length: 10
  convergence_threshold: 0.05
  last_n: 50 #Number of final episodes to compute average and stability.
  settling_tolerance: 0.05
  settling_duration: 100 # Number of steps to consider for settling duration.

test:
  checkpoint_path: experiments\q_learning\pitch_rate_and_theta_error_obs\checkpoint_final_ep60100\checkpoint_ep60099_brain.npz # .npz file

online: 
  fault_type: elevator_loss    # null/elevator_loss
  learn: True
  checkpoint_path: experiments\q_learning\run_20250903_162701\checkpoint_final_ep100\checkpoint_ep99_brain.npz #.npz

optimization:
  target_metric: area_under_curve   # "episodes_to_convergence", "average_final_reward", "success_rate", "area_under_curve", "reward_volatility", "reward_slope", "stability_index"

  metrics: 
    episodes_to_convergence:
      threshold: 0.05
      window: 1000
    episodes_to_convergence_settling_time:
      tolerance_ratio: 0.05
      stability_duration: 100
      last_n: 50
    average_final_reward:
      last_n: 50
    success_rate: {}
    area_under_curve: {}
    reward_volatility:
      window: 50
    reward_slope: {}
    stability_index:
      last_n: 50

  episodes_per_trial: 100
  direction: "maximize"  # "minimize" or "maximize"
  n_trials: 50

  params:
      learning_rate:
        enabled: true
        type: "float"
        low: 0.001
        high: 0.1
        log: true

      gamma:
        enabled: true
        type: "float"
        low: 0.8
        high: 0.999

      epsilon_decay:
        enabled: false
        type: "float"
        low: 0.9
        high: 0.9999

      obs_bins:
        enabled: true
        type: "int"
        low: 5
        high: 50

      action_bins: # bins for elevator defelction
        enabled: true
        type: "int"
        low: 10
        high: 50

