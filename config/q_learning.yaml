agent:
  type: QLearning
  learning_rate: 0.1
  epsilon:
    start: 1.0
    decay: 0.00002
    final: 0.05
  discount_factor: 0.99
  obs_bins: [20, 20]
  action_bins: [1, 20]

env:
  max_steps: 2000
  dt: 0.01
  state_indices_to_keep: [4] # Keep only the relevant states for observation
  reference_config:
    1:
      type: sin 
      #A: 0.698 # 40 degrees in radians
      A: 0.35 # 20 degrees in radians
      T: 5.0
      phi: 0.0
    # 2:
    #   type: constant
    #   value: 1.0
  # Observation space bounds (must match state_indices_to_keep length + errors(ordered))
  #obs_low: [-1.396, -1.215, -2.0, -22.5, -2.0]
  #obs_high: [1.396, 1.205, 2.0, 27.0, 2.0]
  obs_low: [-1.5, -2.5]
  obs_high: [1.5, 2.5]

  # Action space bounds
  action_low: [0, -22.5]
  action_high: [0, 27]


training: 
  episodes: 100
  seed: 42
  
  milestones_fractions:
    - 0.1
    - 0.5
    - 1.0
  
  checkpoint_interval: 10000
  resume_from: null  
  #resume_from: experiments\q_learning\pitch_rate_and_theta_error_obs\checkpoint_final_ep60100\checkpoint_ep60099_brain.npz # null OR .npz file path
  

eval:
  rolling_length: 1000
  convergence_threshold: 0.05
  last_n: 50 #Number of final episodes to compute average and stability.
  settling_tolerance: 0.05
  settling_duration: 100 # Number of steps to consider for settling duration.

test:
  checkpoint_path: experiments\q_learning\run_20250820_002146\checkpoint_final_ep100\checkpoint_ep99_brain.npz # .npz file

online: 
  fault_type: elevator_loss    # null/elevator_loss
  learn: False
  checkpoint_path: experiments\q_learning\pitch_rate_and_theta_error_obs\checkpoint_final_ep60100\checkpoint_ep60099_brain.npz #.npz

optimization:
  target_metric: area_under_curve   # "episodes_to_convergence", "average_final_reward", "success_rate", "area_under_curve", "reward_volatility", "reward_slope", "stability_index"

  metrics: 
    episodes_to_convergence:
      threshold: 0.05
      window: 1000
    episodes_to_convergence_settling_time:
      tolerance_ratio: 0.05
      stability_duration: 100
      last_n: 50
    average_final_reward:
      last_n: 50
    success_rate: {}
    area_under_curve: {}
    reward_volatility:
      window: 50
    reward_slope: {}
    stability_index:
      last_n: 50

  episodes_per_trial: 100
  direction: "maximize"  # "minimize" or "maximize"
  n_trials: 50

  params:
      learning_rate:
        enabled: true
        type: "float"
        low: 0.001
        high: 0.1
        log: true

      gamma:
        enabled: true
        type: "float"
        low: 0.8
        high: 0.999

      epsilon_decay:
        enabled: false
        type: "float"
        low: 0.9
        high: 0.9999

      obs_bins:
        enabled: true
        type: "int"
        low: 5
        high: 50

      action_bins: # bins for elevator defelction
        enabled: true
        type: "int"
        low: 10
        high: 50

