agent:
  type: q_learning
  learning_rate: 0.1
  epsilon:
    start: 1.0
    decay: 0.00002
    final: 0.05
  discount_factor: 0.99
  obs_bins: [20, 20]
  action_bins: [1, 20]

env:
  max_steps: 2000
  dt: 0.01
  state_indices_to_keep: [1] # Keep only the relevant states for observation
  reference_config:
    1:
      type: sin 
      #A: 0.698 # 40 degrees in radians
      A: 0.35 # 20 degrees in radians
      T: 5.0
      phi: 0.0
    # 2:
    #   type: constant
    #   value: 1.0
  # Observation space bounds (must match state_indices_to_keep length + errors(ordered))
  #obs_low: [-1.396, -1.215, -2.0, -22.5, -2.0]
  #obs_high: [1.396, 1.205, 2.0, 27.0, 2.0]
  obs_low: [-1.05, -2.5]
  obs_high: [1.05, 2.5]

  # Action space bounds
  action_low: [0, -22.5]
  action_high: [0, 27]

training:
  episodes: 100
  seed: 42
  milestones_fractions:
 
    # - 0.0
    - 0.5
    # - 0.75
    - 0.9
    - 1.0
  rolling_length: 50
  checkpoint_interval: 10000
  #resume_from: null  # Or: experiments/q_learning/run_20250805_192300/checkpoint_ep200/checkpoint_ep200.json
  resume_from: experiments/q_learning/run_20250805_232035/checkpoint_ep60000/checkpoint_ep60000.json
  

eval:
  rolling_length: 50
  convergence_threshold: 0.05
  last_n: 50 #Number of final episodes to compute average and stability.
  settling_tolerance: 0.05
  settling_duration: 100 # Number of steps to consider for settling duration.



  # input_signal: step
  # test_steps: 3000
