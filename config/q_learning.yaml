agent:
  type: q_learning
  learning_rate: 0.1
  epsilon:
    start: 1.0
    decay: 0.00002
    final: 0.05
  discount_factor: 0.99
  obs_bins: [20, 20]
  action_bins: [1, 20]

env:
  max_steps: 2000
  dt: 0.01
  state_indices_to_keep: [4] # Keep only the relevant states for observation
  reference_config:
    1:
      type: sin 
      #A: 0.698 # 40 degrees in radians
      A: 0.35 # 20 degrees in radians
      T: 5.0
      phi: 0.0
    # 2:
    #   type: constant
    #   value: 1.0
  # Observation space bounds (must match state_indices_to_keep length + errors(ordered))
  #obs_low: [-1.396, -1.215, -2.0, -22.5, -2.0]
  #obs_high: [1.396, 1.205, 2.0, 27.0, 2.0]
  obs_low: [-1.5, -2.5]
  obs_high: [1.5, 2.5]

  # Action space bounds
  action_low: [0, -22.5]
  action_high: [0, 27]

training: 
  episodes: 50100
  seed: 42
  milestones_fractions:
 
    - 0.1
    - 0.5
    - 0.75
    - 0.95
    - 1.0
  checkpoint_interval: 10000
  #resume_from: null  
  resume_from: experiments\q_learning\run_20250807_194916\checkpoint_final_ep10000\checkpoint_ep9999_brain.npz # null OR .npz file path
  

eval:
  rolling_length: 1000
  convergence_threshold: 0.05
  last_n: 50 #Number of final episodes to compute average and stability.
  settling_tolerance: 0.05
  settling_duration: 100 # Number of steps to consider for settling duration.

test:
  checkpoint_path: experiments\q_learning\pitch_rate_and_theta_error_obs\checkpoint_final_ep60100\checkpoint_ep60099_brain.npz # .npz file

