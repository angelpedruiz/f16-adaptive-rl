agent:
  type: q_learning
  learning_rate: 0.1
  epsilon:
    start: 1.0
    decay: 0.002
    final: 0.05
  discount_factor: 0.95
  obs_bins: [20, 20]
  action_bins: [1, 20]

env:
  max_steps: 2000
  dt: 0.01
  state_indices_to_keep: [1] # Keep only the relevant states for observation
  reference_config:
    1:
      type: sin 
      #A: 0.698 # 40 degrees in radians
      A: 0.35 # 20 degrees in radians
      T: 5.0
      phi: 0.0
    # 2:
    #   type: constant
    #   value: 1.0
  # Observation space bounds (must match state_indices_to_keep length + errors(ordered))
  #obs_low: [-1.396, -1.215, -2.0, -22.5, -2.0]
  #obs_high: [1.396, 1.205, 2.0, 27.0, 2.0]
  obs_low: [-1.05, -2.5]
  obs_high: [1.05, 2.5]

  # Action space bounds
  action_low: [0, -22.5]
  action_high: [0, 27]

training:
  episodes: 10
  seed: 42
  milestones_fractions:
    - 0.0
    - 0.5
    - 0.75
    - 0.9
    - 1.0
  rolling_length: 50
  

eval:
  input_signal: step
  test_steps: 3000
