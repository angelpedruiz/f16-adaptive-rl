# DQN Agent Configuration Template
# Deep Q-Network configuration for continuous state spaces

agent:
  type: "dqn"
  
  # Neural network parameters
  hidden_sizes: [64, 64]       # Hidden layer sizes
  learning_rate: 0.001         # Network learning rate
  
  # DQN-specific parameters
  gamma: 0.99                  # Discount factor
  epsilon: 1.0                 # Initial exploration rate
  epsilon_min: 0.01            # Minimum exploration rate
  epsilon_decay: 0.995         # Exploration decay rate
  
  # Experience replay
  memory_size: 100000          # Replay buffer size
  batch_size: 32              # Training batch size
  target_update_freq: 100     # Target network update frequency

environment:
  name: "f16"                  # Environment name
  max_steps: 3000              # Max steps per episode
  dt: 0.01                     # Time step
  
  # State and action configuration
  state_indices_for_obs: [4, 1]  # Observation indices
  action_low: [0, -22.5]       # Action bounds
  action_high: [0, 27.0]
  obs_low: [-2.0, -1.0]        # Observation bounds
  obs_high: [2.0, 1.0]
  
  # Reference configuration
  reference_config:
    1:
      type: "sin"
      A: 0.35
      T: 5.0
      phi: 0.0

training:
  episodes: 5000               # Total episodes
  seed: 42                     # Random seed
  log_frequency: 100           # Logging frequency

checkpointing:
  interval: 1000               # Checkpoint interval
  keep_last_n: 3              # Checkpoints to keep
  save_best: true             # Save best model

plotting:
  training_metrics:
    interval: 200              # Plotting interval
    rolling_window: 100        # Rolling window
    save_individual: true
  
  trajectories:
    episodes: [100, 2500, 5000]  # Trajectory episodes
    save_data: true
  
  figure_size: [12, 8]
  dpi: 150
  style: "default"

evaluation:
  rolling_window: 100
  convergence_threshold: 0.05
  stability_episodes: 50